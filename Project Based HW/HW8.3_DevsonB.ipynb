{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW8.3: Deep Reinforcement Learning - Solve Chung's House 2 problem\n",
    "---\n",
    "###### Name: Devson Butani\n",
    "###### ID: 000732711\n",
    "###### LTU Honor Code: \"I pledge that on all academic work that I submit, I will neither give nor receive unauthorized aid, nor will I present another person's work as my own.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install comet_ml --quiet\n",
    "# %pip install pillow\n",
    "# %pip install matplotlib\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import pylab as plt\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "\n",
    "# Check if GPU available and linked to tensorflow so that keras can use it\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Run DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(13, activation=\"relu\"), input_shape=(1, 13)) # input shape of possible actions\n",
    "    model.add(layers.Dense(56, activation=\"linear\"))\n",
    "    model.add(layers.Dense(13, activation=\"softmax\")) # output shape of possible states\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"mse\",\n",
    "        optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "        metrics=[\"mae\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Data: Reward Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STARTING TABLES\n",
    "R1 = np.array(  # >> CJ's House 2, GOAL: 12\n",
    "    [  #  0, 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12\n",
    "        [-1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1],  # 0\n",
    "        [0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],  # 1\n",
    "        [-1, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1],  # 2\n",
    "        [-1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],  # 3\n",
    "        [0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1],  # 4\n",
    "        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],  # 5 (Blocked)\n",
    "        [-1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1],  # 6\n",
    "        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],  # 7 (Blocked)\n",
    "        [-1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1],  # 8\n",
    "        [-1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1],  # 9\n",
    "        [-1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1],  # 10\n",
    "        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 100],  # 11\n",
    "        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 100],  # 12\n",
    "    ]\n",
    ")\n",
    "Q1 = np.zeros([13, 13])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >> OLD Q-Learning methodology\n",
    "def QLearn(np_reward_table, np_quality_table, goal_state, training_range=900, gamma=0.8):\n",
    "    np_quality_table = np.zeros_like(np_reward_table)  # >> MAKE ZERO ARRAY WITH SAME SHAPE AS REWARD TABLE\n",
    "    scores = []  # >> INIT EMPTY ARRAY TO KEEP TRACK OF LEARNING\n",
    "\n",
    "    for episode in range(training_range):  # >> HOW MANY TIMES TO TRAIN RANDOM PATHS\n",
    "        currSt = random.randint(0, (np.shape(np_reward_table)[0] - 1))  # >> 0 to MAX STATE COUNT\n",
    "        flag = 0  # to handle if initial currSt chosen at randome is 12.\n",
    "\n",
    "        # >> GO UNTILL OUTSIDE THE HOUSE (GOAL STATE) AFTER AT LEAST ONE LOOP\n",
    "        while flag == 0 or currSt != goal_state:\n",
    "            flag = 1\n",
    "            nextSts_fr_currSt = []\n",
    "            nextActs_fr_nextSt = []\n",
    "\n",
    "            # >> FIND ALL POSSIBLE NEXT STATES FOR CURRENT STATE\n",
    "            for index, value in enumerate(np_reward_table[currSt]):\n",
    "                if value != -1:\n",
    "                    nextSts_fr_currSt.append(index)  # >> MAKES A 1-D ARRAY OF POSSIBLE STATES\n",
    "\n",
    "            # >> SELECT ONE FROM ABOVE TO BE NEXT STATE\n",
    "            nextSt = random.choice(np.array(nextSts_fr_currSt))  # choice(): we can get a random sample from 1Darray\n",
    "\n",
    "            # >> FIND ALL POSSIBLE ACTIONS FROM NEXT STATE\n",
    "            for index, value in enumerate(np_reward_table[nextSt]):\n",
    "                if value != -1:\n",
    "                    nextActs_fr_nextSt.append(index)  # >> MAKES A 1-D ARRAY OF POSSIBLE ACTIONS\n",
    "\n",
    "            # >> FIND THE MAX YEILDING ACTION FROM ABOVE ARRAY AND ADJUST THE PATH TO BE TAKEN'S WORTH\n",
    "            np_quality_table[currSt, nextSt] = np_reward_table[currSt, nextSt] + gamma * max(\n",
    "                np_quality_table[nextSt, nextActs_fr_nextSt]\n",
    "            )\n",
    "            # >> RUN IT AGAIN FOR THE NEW CURRENT STATE UNTIL GOAL STATE\n",
    "            currSt = nextSt\n",
    "\n",
    "        # >> MAKES A 1-D ARRAY OF CURRENT Q WITH RESPECT TO MAX(Q) UP TILL NOW. THIS ALLOWS GRAPHING THE LEARNING CURVE FOR EACH EPISODE\n",
    "        if np.max(np_quality_table) > 0:\n",
    "            scores.append(np.sum(np_quality_table / np.max(np_quality_table) * 100))\n",
    "        else:\n",
    "            scores.append(0)\n",
    "\n",
    "    # >> RESULTING RAW Q TABLE\n",
    "    # print(np.array_str(np_quality_table, precision=1), end='\\n\\n')\n",
    "    # >> Q TABLE AS PERCENTAGE OF MAX(Q) - ROUNDED AFTER PERCENTAGE\n",
    "    np_quality_table = np.round(\n",
    "        (np_quality_table * 100 / np.max(np_quality_table))\n",
    "    )  # >> ROUNDING PRECESION CAN BE CHANGED DEPENDING ON APPLICATION\n",
    "    print(f\"Quality Table = \")\n",
    "    print(f\"{np_quality_table}\")\n",
    "\n",
    "    # >> PLOT THE LEARNING CURVE USING SCORES GATHERED FOR EVERY TRAINING EPISODE\n",
    "    plt.plot(scores)\n",
    "    plt.show()\n",
    "\n",
    "    return np_quality_table\n",
    "\n",
    "\n",
    "# >> New Q-Learning model\n",
    "def DeepQLearn(np_reward_table, np_quality_table, goal_state, training_range=900, gamma=0.8):\n",
    "    np_quality_table = np.zeros_like(np_reward_table)  # >> MAKE ZERO ARRAY WITH SAME SHAPE AS REWARD TABLE\n",
    "    scores = []  # >> INIT EMPTY ARRAY TO KEEP TRACK OF LEARNING\n",
    "\n",
    "    for episode in range(training_range):  # >> HOW MANY TIMES TO TRAIN RANDOM PATHS\n",
    "        currSt = random.randint(0, (np.shape(np_reward_table)[0] - 1))  # >> 0 to MAX STATE COUNT\n",
    "\n",
    "    # Train per piece inside a for loop? with current state as input and reward[state] as labels\n",
    "    # Batch size is path length\n",
    "    # Epoch is training_range\n",
    "    # Loss function = custom equation\n",
    "    # Output is model not path or table. Use model to predict Q table to show? \n",
    "    # Or just show path taken when predict? Callbacks can help with this.\n",
    "\n",
    "    # >> RESULTING RAW Q TABLE\n",
    "    # print(np.array_str(np_quality_table, precision=1), end='\\n\\n')\n",
    "    # >> Q TABLE AS PERCENTAGE OF MAX(Q) - ROUNDED AFTER PERCENTAGE\n",
    "    np_quality_table = np.round(\n",
    "        (np_quality_table * 100 / np.max(np_quality_table))\n",
    "    )  # >> ROUNDING PRECESION CAN BE CHANGED DEPENDING ON APPLICATION\n",
    "    print(f\"Quality Table = \")\n",
    "    print(f\"{np_quality_table}\")\n",
    "\n",
    "    # >> PLOT THE LEARNING CURVE USING SCORES GATHERED FOR EVERY TRAINING EPISODE\n",
    "    plt.plot(scores)\n",
    "    plt.show()\n",
    "\n",
    "    return np_quality_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
