{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Term Project: Detect Stop Signs in various conditions using Deep Learning\n",
        "---\n",
        "#### Name: Devson Butani (and Kim Lam, Sydney Ross)\n",
        "#### ID: 000732711\n",
        "#### LTU Honor Code: \"I pledge that on all academic work that I submit, I will neither give nor receive unauthorized aid, nor will I present another person's work as my own.\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "# Setup Dependencies"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cuda and PyTorch are version matched for RTX 30-series GPUs. Latest versions are okay on Collab but don't work on PCs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!py -m pip install --upgrade setuptools pip wheel --quiet\n",
        "!py -m pip install nvidia-pyindex --quiet\n",
        "!py -m pip install nvidia-cuda-runtime-cu11 --quiet\n",
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WAq73VLWLtD",
        "outputId": "8afe6620-f578-4f38-8a75-83c998b93af9"
      },
      "outputs": [],
      "source": [
        "# Add Comet ML for data logging and tracking models\n",
        "# %pip install comet_ml --quiet\n",
        "import comet_ml\n",
        "comet_ml.config.save(api_key=\"mQStuXAxGmHmK1vOsTRucvz76\") # Insert API key from comet user account\n",
        "comet_ml.init(project_name='Stop_sign_detection_using_yolov8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwE6rOO3y5Y3",
        "outputId": "9a527a0a-7729-457a-99de-dc57913db58e"
      },
      "outputs": [],
      "source": [
        "# Install ultralytics (YOLOv8)\n",
        "# %pip install ultralytics --quiet\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import PyTorch and verify GPU is connected correctly through CUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgLLbKmotcJU",
        "outputId": "7c4bcb77-9b5b-4c89-b60d-4a0b70e0a8cb"
      },
      "outputs": [],
      "source": [
        "# !pip3 install torch --index-url https://download.pytorch.org/whl/cu118 --quiet    \n",
        "# !nvidia-smi\n",
        "import torch\n",
        "print(torch.cuda.is_available()) # check if PyTorch can see it\n",
        "print(torch.cuda.get_device_name(0)) # confirm the device name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch.cuda.set_per_process_memory_fraction(0.6, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a3NaCnVJDVh"
      },
      "source": [
        "# Get dataset from `Google Drive` OR `Roboflow`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "0YkmTmk5MK6i",
        "outputId": "37523f7f-e5dc-433f-9c0e-acbe21737470"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change directory to a drive folder of choice - Runs and models will be saved here\n",
        "%cd '/content/drive/MyDrive/DL_data'\n",
        "%pwd # Verify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgBuc4ytK3-1",
        "outputId": "0d458aba-82c9-42fc-a8dc-65e3f595c73d"
      },
      "outputs": [],
      "source": [
        "# Use this to select between Roboflow or already downloaded dataset\n",
        "select = 0\n",
        "\n",
        "if select == 1:\n",
        "  ### Download from Roboflow\n",
        "  !pip install roboflow\n",
        "  from roboflow import Roboflow\n",
        "  rf = Roboflow(api_key=\"x7krcZVIF3tFJqZdd6VJ\")\n",
        "  project = rf.workspace(\"yolo-ifyjn\").project(\"stop-sign-detection-2\")\n",
        "  dataset = project.version(2).download(\"yolov8\")\n",
        "  data_path = f\"{dataset.location}/data.yaml\"\n",
        "else: \n",
        "  ### Use Dataset inside a Google Drive folder\n",
        "  # Find path from sidebar on the left\n",
        "  # Find the .yaml file and copy path from it's options (three dots)\n",
        "  # \"Devson's\"\n",
        "  # data_path = '/content/drive/MyDrive/DL_data/Stop_sign_1500/data.yaml'\n",
        "  data_path = r\"c:\\Users\\devso\\Desktop\\YOLO_Train\\StopSignDetection-1-4\\data.yaml\"\n",
        "  # DS needs to be added to MyDrive. Shared folders do not work directly.\n",
        "\n",
        "print(f\"data_path = {data_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK5kIgomZS41"
      },
      "source": [
        "# Train `Yolov8` model:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Batch size and image size have huge impact on GPU memory usage\n",
        "* Batches lower than 8 result in poor performance (less than 70% accuracy)\n",
        "* Images smaller than 640 do not see the stop sign within 20ft\n",
        "* Camera resolution on the vehicle is around 1536px\n",
        "* Using native image resolution, only the nano model can be used at batch size 16. (sees 35ft far)\n",
        "* Overfitting usually happens between 15 and 35 epochs for most YOLO models trained\n",
        "* Medium model performs roughly 20% better but uses all the memory available in the ACTOR1 laptop\n",
        "* Medium model takes around 45ms per prediction without displaying the image. (Nano: 4ms)\n",
        "* Default YOLOv8m model trained on the COCO dataset is really good (90% acc) at detecting clear stop signs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BQjrO6SAZhep",
        "outputId": "b08e39a0-d6ed-4cf8-f422-5a8fd52e1cad"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "model = YOLO('yolov8m.yaml') # build from untrained YAML parameters\n",
        "results = model.train(data=data_path, batch=16, epochs=25, imgsz=640, device=\"0\", cache=False, val=True) # train the model over our dataset\n",
        "# results = model.val() # can specify data='' to verify the model with\n",
        "# model.export() # Save Model is set to auto but can be changed if required"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">Run logs can be found on CometML at \n",
        "https://www.comet.com/aeolus96/stop-sign-detection-using-yolov8/a3e5c5ce2c3f478c942fbabdc026d1ee?experiment-tab=panels&showOutliers=true&smoothing=0&xAxis=step OR `arbitrary_lamprey_5982` at https://www.comet.com/aeolus96/stop-sign-detection-using-yolov8/\n",
        "* PPT attached as a guide to CometML\n",
        "* This .ipynb file does not have terminal output due to the last few runs not working correctly. However, output of the last epoch and testing results are available under the output tab on Comet"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training is complete, rest of the code is for testing bits and pieces for implementing in ROS\n",
        "- - -\n",
        "ROS package too big for canvas upload: https://github.com/Aeolus96/Route-StopSignDetector.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "model = YOLO(r\"C:\\Users\\devso\\Desktop\\YOLO_Train\\n_best.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = model(source=r\"C:\\Users\\devso\\Desktop\\YOLO_Train\\test_5.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(results[0])\n",
        "img = results[0].plot()\n",
        "# print(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# boxes = results[0].boxes.cpu().numpy()\n",
        "# print(box[1].xywh[0][2])\n",
        "# print(box[2]*box[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "cv2.imshow(\"img\", img)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use results to determine class and size\n",
        "# Get the bounding boxes and class labels for all detected objects\n",
        "boxes = results[0].boxes.cpu().numpy() # Only cast boxes to numpy\n",
        "labels = results[0].names # Direct class labels access {0: 'stop-sign', 1: 'stop-sign-fake', 2: 'stop-sign-obstructed', 3: 'stop-sign-vandalized'}\n",
        "# Iterate over all detected bounding boxes\n",
        "at_least_one_stop_sign = False # Used to pass results to the next function\n",
        "biggest_bounding_box = 0 # Used to pass results to the next function\n",
        "for idx, box in enumerate(boxes):\n",
        "    # Get the class label\n",
        "    label = labels[int(box.cls)]\n",
        "    # Check if this object is a stop sign\n",
        "    if label == 'stop-sign' or label == 'stop-sign-obstructed' or label == 'stop-sign-vandalized':\n",
        "        at_least_one_stop_sign = True\n",
        "        # Find the width and height of the bounding box\n",
        "        width = box.xywh[0][2]\n",
        "        height = box.xywh[0][3]\n",
        "        area = int(width * height)\n",
        "        # Print the location and size of the stop sign\n",
        "        print(f\"[{label}] detected at ({box.xywh[0][0]:.1f}, {box.xywh[0][1]:.1f}) with size {area:.0f} sq_pix\")\n",
        "        if area > biggest_bounding_box:\n",
        "                biggest_bounding_box = area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(at_least_one_stop_sign)\n",
        "print(biggest_bounding_box)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "detection_size = 1536\n",
        "def resize_image(img, size=(detection_size, detection_size)):\n",
        "    h, w = img.shape[:2]\n",
        "    c = img.shape[2] if len(img.shape) > 2 else 1\n",
        "    if h == w:\n",
        "        return cv2.resize(img, size, cv2.INTER_AREA)\n",
        "    dif = h if h > w else w\n",
        "    interpolation = cv2.INTER_AREA if dif > (size[0] + size[1]) // 2 else cv2.INTER_CUBIC\n",
        "    x_pos = (dif - w) // 2\n",
        "    y_pos = (dif - h) // 2\n",
        "    if len(img.shape) == 2:  # Grayscale images\n",
        "        mask = np.zeros((dif, dif), dtype=img.dtype)\n",
        "        mask[y_pos : y_pos + h, x_pos : x_pos + w] = img[:h, :w]\n",
        "    else:  # 3-channel color images\n",
        "        mask = np.zeros((dif, dif, c), dtype=img.dtype)\n",
        "        mask[y_pos : y_pos + h, x_pos : x_pos + w, :] = img[:h, :w, :]\n",
        "    return cv2.resize(mask, size, interpolation)\n",
        "from ultralytics import YOLO\n",
        "\n",
        "\n",
        "# Use default YOLOv8m model trained on the COCO dataset to find bounding box of the stop sign\n",
        "coco_model = YOLO(r\"C:\\Users\\devso\\Desktop\\YOLO_Train\\yolov8m.pt\")\n",
        "model = YOLO(r\"C:\\Users\\devso\\Desktop\\YOLO_Train\\n_best.pt\")\n",
        "coco_results = coco_model(source=r\"C:\\Users\\devso\\Desktop\\YOLO_Train\\test_10.jpg\", # Image source\n",
        "                classes=11, # only detect class name \"stop sign\" from COCO dataset\n",
        "                # iou=0.5,\n",
        "                # agnostic_nms=True,\n",
        "                device=\"0\") # Use GPU for inference\n",
        "original_img = coco_results[0].orig_img\n",
        "# results_img = coco_results[0].plot()\n",
        "# cv2.imshow(\"YOLO-COCO-detection\", results_img)\n",
        "coco_boxes = coco_results[0].boxes.cpu().numpy() # Only cast boxes to numpy\n",
        "at_least_one_stop_sign = False # Used to pass results to the next function\n",
        "biggest_bounding_box = 0 # Used to pass results to the next function\n",
        "for idx, box in enumerate(coco_boxes): # For every \"stop sign\" like object\n",
        "    # Find the width and height of the bounding box\n",
        "    x1 = int(box.xyxy[0][0])\n",
        "    y1 = int(box.xyxy[0][1])\n",
        "    x2 = int(box.xyxy[0][2])\n",
        "    y2 = int(box.xyxy[0][3])\n",
        "    box_img = original_img[y1:y2, x1:x2]\n",
        "    box_img = resize_image(box_img)\n",
        "    cv2.imshow(\"idx\", box_img)\n",
        "    cv2.waitKey(0)\n",
        "    \n",
        "    # Use the bounding box to determine the class of the stop sign\n",
        "    results = model(source=box_img, imgsz=640, agnostic_nms=True, device=\"0\")\n",
        "    # Get the bounding boxes and class labels for all detected objects\n",
        "    boxes = results[0].boxes.cpu().numpy() # Only cast boxes to numpy\n",
        "    labels = results[0].names # Direct class labels access\n",
        "    # {0: 'stop-sign', 1: 'stop-sign-fake', 2: 'stop-sign-obstructed', 3: 'stop-sign-vandalized'}\n",
        "    for idx, box in enumerate(boxes):\n",
        "        label = labels[int(box.cls)] # Get the class label\n",
        "        # Check if this object is an actual stop sign\n",
        "        if label == 'stop-sign' or label == 'stop-sign-obstructed' or label == 'stop-sign-vandalized':\n",
        "            at_least_one_stop_sign = True\n",
        "            # Find the width and height of the bounding box\n",
        "            width = box.xywh[0][2]\n",
        "            height = box.xywh[0][3]\n",
        "            area = int(width * height)\n",
        "            # Print the location and size of the stop sign\n",
        "            print(f\"[{label}] detected at ({box.xywh[0][0]:.1f}, {box.xywh[0][1]:.1f}) with size {area:.0f} sq_pix\")\n",
        "            if area > biggest_bounding_box:\n",
        "                biggest_bounding_box = area\n",
        "\n",
        "\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(label)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
